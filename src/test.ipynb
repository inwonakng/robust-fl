{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import numpy as np\n",
    "from simulator import Simulator\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "agg_config = yaml.safe_load(open('../configurations/aggregators/fedavg.yaml'))\n",
    "client_config = yaml.safe_load(open('../configurations/clients/no_poison_no_straggle.yaml'))\n",
    "data_config = yaml.safe_load(open('../configurations/datasets/mnist_0_1.yaml'))\n",
    "\n",
    "sim = Simulator(\n",
    "    **agg_config,\n",
    "    **client_config,\n",
    "    **data_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0222, 0.0444, 0.0667, 0.0889, 0.1111, 0.1333, 0.1556, 0.1778,\n",
       "        0.2000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aggregators.utils import normalize_weights\n",
    "\n",
    "normalize_weights(torch.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sim.global_model.model.parameters())[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acacf3111254d1bada6080d17b05d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running None:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [784] at entry 0 and [] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     avg_test_acc \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(test_acc_scores) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(test_acc_scores)\n\u001b[1;32m     18\u001b[0m     \u001b[39m# break\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m#     # Step 4. Update the global model with the finished local updates\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     new_state \u001b[39m=\u001b[39m sim\u001b[39m.\u001b[39;49maggregator(sim\u001b[39m.\u001b[39;49mglobal_model, to_update_global)\n\u001b[1;32m     22\u001b[0m \u001b[39m#     if new_state is not None:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#         sim.global_model.set_state(new_state)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[39m# pred = sim.global_model.predict(sim.x_test)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/research/robust-fl/src/aggregators/aggregators.py:38\u001b[0m, in \u001b[0;36mAggregator.__call__\u001b[0;34m(self, global_model, updates)\u001b[0m\n\u001b[1;32m     36\u001b[0m to_update_global \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(updates) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     to_update_global \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggregate(global_model, updates)\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate(to_update_global)\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m to_update_global\n",
      "File \u001b[0;32m~/Documents/research/robust-fl/src/aggregators/fedavg.py:26\u001b[0m, in \u001b[0;36mFedAvg.aggregate\u001b[0;34m(self, global_model, updates)\u001b[0m\n\u001b[1;32m     23\u001b[0m model_weights, update_weights, update_delays \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_updates(updates)\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m key, components \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(new_global_state, model_weights):\n\u001b[0;32m---> 26\u001b[0m     new_global_state[key] \u001b[39m=\u001b[39m weighted_average(components, update_weights)\n\u001b[1;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m new_global_state\n",
      "File \u001b[0;32m~/miniconda3/envs/robust-fl/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/research/robust-fl/src/aggregators/utils.py:38\u001b[0m, in \u001b[0;36mweighted_average\u001b[0;34m(model_weights, update_weights)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweighted_average\u001b[39m(\n\u001b[1;32m     33\u001b[0m     model_weights: Union[List[List[torch\u001b[39m.\u001b[39mTensor]], List[torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor], \n\u001b[1;32m     34\u001b[0m     update_weights: torch\u001b[39m.\u001b[39mTensor\n\u001b[1;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[List[torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m     36\u001b[0m     \u001b[39m# normalize update_weights so we don't have to divid later. If they are already normalized it does nothing.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     update_weights \u001b[39m=\u001b[39m normalize_weights(update_weights)\n\u001b[0;32m---> 38\u001b[0m     w_avg \u001b[39m=\u001b[39m [\n\u001b[1;32m     39\u001b[0m         \u001b[39m# each point has two dimensions, and by stacking them we get 3 dimensions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         \u001b[39m# instead of doing nasty transposing, we can just add matching dims for the weight\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         (\n\u001b[1;32m     42\u001b[0m             torch\u001b[39m.\u001b[39mstack(p) \n\u001b[1;32m     43\u001b[0m             \u001b[39m*\u001b[39m update_weights\u001b[39m.\u001b[39mreshape([\u001b[39mlen\u001b[39m(model_weights)] \u001b[39m+\u001b[39m [\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m p[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mndim)\n\u001b[1;32m     44\u001b[0m         )\u001b[39m.\u001b[39msum(\u001b[39m0\u001b[39m)\n\u001b[1;32m     45\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mmodel_weights)\n\u001b[1;32m     46\u001b[0m     ]\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m w_avg\n",
      "File \u001b[0;32m~/Documents/research/robust-fl/src/aggregators/utils.py:42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweighted_average\u001b[39m(\n\u001b[1;32m     33\u001b[0m     model_weights: Union[List[List[torch\u001b[39m.\u001b[39mTensor]], List[torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor], \n\u001b[1;32m     34\u001b[0m     update_weights: torch\u001b[39m.\u001b[39mTensor\n\u001b[1;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[List[torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m     36\u001b[0m     \u001b[39m# normalize update_weights so we don't have to divid later. If they are already normalized it does nothing.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     update_weights \u001b[39m=\u001b[39m normalize_weights(update_weights)\n\u001b[1;32m     38\u001b[0m     w_avg \u001b[39m=\u001b[39m [\n\u001b[1;32m     39\u001b[0m         \u001b[39m# each point has two dimensions, and by stacking them we get 3 dimensions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         \u001b[39m# instead of doing nasty transposing, we can just add matching dims for the weight\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         (\n\u001b[0;32m---> 42\u001b[0m             torch\u001b[39m.\u001b[39;49mstack(p) \n\u001b[1;32m     43\u001b[0m             \u001b[39m*\u001b[39m update_weights\u001b[39m.\u001b[39mreshape([\u001b[39mlen\u001b[39m(model_weights)] \u001b[39m+\u001b[39m [\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m p[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mndim)\n\u001b[1;32m     44\u001b[0m         )\u001b[39m.\u001b[39msum(\u001b[39m0\u001b[39m)\n\u001b[1;32m     45\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mmodel_weights)\n\u001b[1;32m     46\u001b[0m     ]\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m w_avg\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [784] at entry 0 and [] at entry 1"
     ]
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "\n",
    "for epoch in tqdm(\n",
    "    range(n_epoch), \n",
    "    desc=f'Running {str(sim.output_dir)}', \n",
    "    leave=True,\n",
    "):\n",
    "    picked_clients, to_update_global = sim.step()\n",
    "    avg_losses = [u.avg_loss for u in to_update_global]\n",
    "    train_acc_scores = [u.train_acc_score for u in to_update_global]\n",
    "    test_acc_scores = [u.test_acc_score for u in to_update_global]\n",
    "\n",
    "    if len(to_update_global):\n",
    "        # only update the global model if we have any updates\n",
    "        avg_train_acc = sum(train_acc_scores) / len(train_acc_scores)\n",
    "        avg_test_acc = sum(test_acc_scores) / len(test_acc_scores)\n",
    "\n",
    "        # break\n",
    "    #     # Step 4. Update the global model with the finished local updates\n",
    "        new_state = sim.aggregator(sim.global_model, to_update_global)\n",
    "\n",
    "    #     if new_state is not None:\n",
    "    #         sim.global_model.set_state(new_state)\n",
    "    # else:\n",
    "    #     avg_train_acc = 0\n",
    "    #     avg_test_acc = 0\n",
    "    \n",
    "    # pred = sim.global_model.predict(sim.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([200, 784]), torch.Size([200, 784]), torch.Size([200, 784]), torch.Size([200, 784]), torch.Size([200, 784]), torch.Size([200, 784]), torch.Size([200, 784]), torch.Size([200, 784]), torch.Size([200, 784]), torch.Size([200, 784])]\n",
      "[torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200])]\n",
      "[torch.Size([200, 200]), torch.Size([200, 200]), torch.Size([200, 200]), torch.Size([200, 200]), torch.Size([200, 200]), torch.Size([200, 200]), torch.Size([200, 200]), torch.Size([200, 200]), torch.Size([200, 200]), torch.Size([200, 200])]\n",
      "[torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200]), torch.Size([200])]\n",
      "[torch.Size([2, 200]), torch.Size([2, 200]), torch.Size([2, 200]), torch.Size([2, 200]), torch.Size([2, 200]), torch.Size([2, 200]), torch.Size([2, 200]), torch.Size([2, 200]), torch.Size([2, 200]), torch.Size([2, 200])]\n",
      "[torch.Size([2]), torch.Size([2]), torch.Size([2]), torch.Size([2]), torch.Size([2]), torch.Size([2]), torch.Size([2]), torch.Size([2]), torch.Size([2]), torch.Size([2])]\n"
     ]
    }
   ],
   "source": [
    "from aggregators.utils import weighted_average\n",
    "new_global_state = sim.global_model.get_state()\n",
    "model_weights, update_weights, update_delays = sim.aggregator.parse_updates(to_update_global)\n",
    "\n",
    "\n",
    "for key, components in zip(new_global_state, zip(*model_weights)):\n",
    "    print([c.shape for c in components])\n",
    "    new_global_state[key] = weighted_average(components, update_weights)\n",
    "\n",
    "    # break\n",
    "# return new_global_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ -0.3369,  24.1400, -37.0370,  ...,  27.6388,   4.7031,   2.5960],\n",
       "         [-24.9191, -18.8764, -13.1574,  ..., -25.5334,  -7.5126, -37.7317],\n",
       "         [-25.3871,  18.7722, -41.9277,  ..., -25.6333,   1.4889,  10.1148],\n",
       "         ...,\n",
       "         [ -0.3475, -33.4996, -25.0153,  ...,  -7.8178,  18.8428,  42.9558],\n",
       "         [ 28.3070, -38.4243, -40.7563,  ..., -37.4338,  -8.5625,  21.0292],\n",
       "         [ 44.7266,  41.3369,  43.6056,  ...,   6.2606,  10.3698,  18.8232]]),\n",
       " tensor([ 27.7479,  28.2241, -42.1896,  18.0164,  24.8717,  36.1386,   3.0739,\n",
       "          27.7637, -16.3844, -11.1367,  -7.3653, -24.7901, -36.1190,  42.2010,\n",
       "          27.4532, -17.6969, -10.9478,  37.2056,  31.7967,  28.0664,  34.3884,\n",
       "         -14.2719, -21.5954,  -7.2163,  16.3790,  44.4648,  -6.9938,  31.8804,\n",
       "          19.9674,  11.0826,   1.7534, -23.8572,  34.2364,  -3.7545,  32.1327,\n",
       "         -38.4016, -24.2678,  34.2979,  12.2915, -37.1306,  16.8781,  33.5338,\n",
       "         -24.6969,  -8.1868,  41.0820, -25.6336,  42.9846, -41.5971,  -1.5086,\n",
       "          30.3996,   5.8374, -40.8908,  31.6103, -24.6664, -41.4824, -20.7797,\n",
       "          34.9066,  -5.5344, -36.1095, -22.8432,  -5.1861,   9.1612,  45.1903,\n",
       "         -13.7248,  -2.4401,  37.9370,  37.0444, -23.6808, -40.2394,   8.1836,\n",
       "          -8.9491,  -9.6624,   5.4103,  31.2584,  13.9558,  -2.3231, -18.5280,\n",
       "         -11.2013, -17.6728, -23.6036, -37.8081, -28.2067,  18.6423,   5.5021,\n",
       "          -0.7489,  38.8821,  20.8238, -43.5794,  30.4776, -44.8507, -17.4307,\n",
       "          10.8081, -17.3327, -27.9812,  -4.5748,  -7.4255, -15.1950, -30.4198,\n",
       "           7.4765,  13.4805,  -7.7645,  22.8802,  40.3124, -20.7450, -25.9956,\n",
       "         -21.6824, -33.0349, -35.4843,   7.5460, -22.5983,   6.3932,  -1.8688,\n",
       "          42.0028,  37.2078, -27.8070,  16.7395,   0.6204,  27.6963,  24.7705,\n",
       "          36.7931,   5.8642, -39.9325, -24.1162,  30.6238,  45.0665, -31.1891,\n",
       "          38.5780,  26.9465, -13.5453, -24.1317,  36.4828, -41.3610, -18.3258,\n",
       "          40.9911, -19.5182, -35.2545,  -6.6888,  -4.0250,  -6.3800, -23.0339,\n",
       "          22.2087,  44.0729,  23.4947, -20.1435,   6.0484, -38.2153,  13.8319,\n",
       "          26.9435,   2.6249,  21.3595,   4.3566, -16.5774,  13.3580, -26.7782,\n",
       "           0.2056,   0.7391,  -2.5616,  39.5213,  -0.8845,  -7.3382, -12.3178,\n",
       "         -33.4364,  22.3076,  41.4909,  38.1979,  14.9112,  -7.3272, -43.2634,\n",
       "         -29.2746, -41.8922,  24.2194, -40.9387,   1.6643,  42.1072, -30.1765,\n",
       "          19.5206,  14.8604,  24.5585, -36.1859,  39.7748,  36.8927,  16.8897,\n",
       "         -42.0422,  32.9490,  -1.0038,  17.6043,  34.5309, -27.6738,  -2.2349,\n",
       "          23.4322, -42.7795, -18.9303,   1.9592,  14.7429,  17.6184,  19.1640,\n",
       "          34.1237,  37.4228,  31.5411, -26.7498]),\n",
       " tensor([[-26.9020, -35.8483,  27.6234,  ...,   7.3172,  -6.4082,   2.7469],\n",
       "         [ 63.4274,   3.5427,  -7.9875,  ...,  84.5434,  51.9500,  19.1588],\n",
       "         [-87.5356, -59.2905,  53.7928,  ..., -71.9995, -40.6033, -41.4732],\n",
       "         ...,\n",
       "         [ 57.7584,   9.4467, -37.9021,  ..., -85.5176,  15.1134,  11.3725],\n",
       "         [  8.8352,  10.9286,  48.1991,  ...,  67.8933,  28.9066,  53.3412],\n",
       "         [ 51.5662,  56.9939,  46.6424,  ...,  43.7220, -49.6628,  17.8696]]),\n",
       " tensor([ 26.8789, -40.6073,  43.8145, -12.3605, -25.3516, -22.8620, -18.8577,\n",
       "          -0.3915,  -3.0836,  74.6147, -34.8910, -52.3906,  17.2928, -35.2571,\n",
       "         -61.6074,  33.6529, -50.2917,  -7.6663, -39.8427,  53.4507,  18.7613,\n",
       "           6.3797, -49.1671, -74.4281, -61.5368, -55.0264,  38.4748,  20.4692,\n",
       "          44.0747, -50.0169,  41.0270, -18.9235,   0.9717,   1.2302, -45.1065,\n",
       "         -65.6912,  -7.6733, -82.2301,  42.2010,  66.8502,  34.5043, -40.4311,\n",
       "           1.7071,  51.6276,   1.4191,  78.7650, -57.2183, -88.7006,  49.2443,\n",
       "         -79.4726,  68.8717,  56.1024,  44.7052,  46.7580,  21.3811,  67.1144,\n",
       "         -76.3583, -52.8055, -30.4826,   3.2510, -63.8793,  14.3992,  11.9897,\n",
       "          82.0772,  80.6476,  66.9651, -87.8716, -37.7786, -59.1207,  47.0828,\n",
       "          -5.5026,  66.9114,  34.9472, -88.6486,  74.0644, -36.0615, -39.4009,\n",
       "          71.4422, -76.5957, -65.6203,  70.6148, -74.7459, -24.3162,  85.9414,\n",
       "         -33.4859, -71.4563, -70.1827, -57.5432, -57.8147, -57.9553, -43.7811,\n",
       "         -43.2886,  67.0008, -62.0270,  34.8350, -66.3255, -59.6963, -63.5151,\n",
       "         -74.2550,  37.9582, -76.8409, -69.3900,  58.5577,  23.0103, -22.1552,\n",
       "          23.2553, -17.1706, -40.4407, -72.3494,  78.5452, -11.6191, -70.1382,\n",
       "          53.3794, -77.5084,  56.9461,  49.6639,  26.2809,  45.5482, -53.2896,\n",
       "         -76.8559, -25.3626,  28.5830,  -2.5750, -38.5143, -81.4949,  37.0226,\n",
       "           9.5438, -30.6048,  33.2694, -79.0144, -83.5948,  33.7527, -28.8135,\n",
       "          44.9683,  67.3358, -14.5934, -70.3436, -23.0818,  14.5666,  27.4402,\n",
       "         -30.4528,   9.6207,  -0.5630,  76.1998, -77.1720, -53.9547, -29.2878,\n",
       "          31.9795, -74.8101,  34.7528,  78.3144, -74.1766, -86.3611, -46.8002,\n",
       "         -87.4485, -87.7169, -36.4017,  53.7540,  16.3251, -12.8147, -31.3315,\n",
       "          72.2445, -62.6337,  66.0775, -60.7874,  64.9025,  48.8501,  39.6448,\n",
       "           4.2143,  61.3840,  83.7837,  21.3512,   5.7628, -37.1225, -86.7940,\n",
       "         -16.5417,  53.4374, -56.1272,  27.3872,   2.9378,  -4.3341, -34.9061,\n",
       "         -70.2788,  -9.9524,  -8.8617, -68.1865, -43.5267, -13.3894,   7.5139,\n",
       "          12.7479,  81.1999, -21.5790,  29.9351,  22.3585, -40.9871, -49.9895,\n",
       "         -81.9651, -35.5700, -30.2326, -71.8353]),\n",
       " tensor([[-27.0418, -40.3218,  54.3797,   1.4216, -64.6989, -80.6079,  48.6054,\n",
       "          -82.0170, -88.5365,  83.3714, -34.1228, -69.6856,  69.8083,  37.6559,\n",
       "           16.2216, -78.0543,  70.0304,  17.9100,  14.4028, -69.7077, -59.6568,\n",
       "           -7.4965, -65.0688,  41.7761, -48.5799,  57.1739, -75.4883,  23.6722,\n",
       "           59.6106,  35.2569,  60.9340,  43.6257,  -6.6939, -37.2076,  65.8526,\n",
       "          -25.7828, -30.4117,  24.6463, -16.0156, -63.9410, -69.5845, -43.8703,\n",
       "          -86.1731, -71.0697,  70.0664, -32.1700,  77.7732, -71.6498,  22.4051,\n",
       "          -84.0366, -37.1442, -65.7393,   0.1486,  79.0374, -12.1583,  59.9686,\n",
       "          -86.5264, -33.7734,  59.5022, -41.9304,  69.1789, -20.3671, -56.1976,\n",
       "          -64.0608, -49.1678, -10.0985, -19.5951,  15.0536,  75.8723,  47.0463,\n",
       "            7.2698,  61.1355, -39.7329,  84.7919,  21.1999,  72.3970, -73.6550,\n",
       "            3.8508, -66.1371,  87.0898, -65.3364, -34.1475,  55.5951, -42.7688,\n",
       "           18.4592,  80.2680, -71.1108, -15.5458, -88.4126,  78.1793, -33.4191,\n",
       "          -28.3073, -67.7234,  25.3922, -57.6771,  -0.4145,  -7.4713,  82.0462,\n",
       "           12.0952,  81.7787,  86.4102,  20.8555,  63.9564,  71.1427,  20.1228,\n",
       "          -19.2635,  72.8662, -33.1177, -35.0753,  42.5853,  37.6979,  36.5118,\n",
       "          -29.5070, -53.1747,  42.1440, -33.7793,  23.5200,  21.3980,  17.9460,\n",
       "          -46.1195, -87.3784, -15.4318, -36.9277, -70.6845, -39.2239,  65.4810,\n",
       "          -46.6561, -85.2223, -76.5504,  71.1507, -82.1170,   1.5423,  13.6609,\n",
       "          -81.1535,  25.3277,  63.6016,  50.4600,  35.3729, -18.3539, -60.6070,\n",
       "           78.6522,  82.8666,  21.2709,  58.4587, -78.1782, -31.6257,  67.4109,\n",
       "           54.3251,  56.2706,  36.3988, -71.8353,  64.7712,  48.9971,  16.7307,\n",
       "           27.5062,  27.1692, -54.6522, -19.2412,  -0.1151,   6.6734, -64.0708,\n",
       "          -43.6666,  12.3000, -56.8067, -15.2343,  28.9925,  72.2073,  44.2147,\n",
       "           77.5139,  48.4798,  55.4234, -12.1880,  46.9907,  43.9328, -70.7868,\n",
       "          -25.1450, -78.6060, -25.9719,  37.3339, -32.7447,  65.1261,  11.4210,\n",
       "          -29.2522,  81.1422, -83.1610,   1.1094,  -9.2063,  -5.6797,  39.5977,\n",
       "           33.1565,  -2.7691,  35.7107,  71.4962, -38.9443,   9.3318,  -6.5936,\n",
       "          -78.4988, -17.6936,   1.4087,  -4.8806],\n",
       "         [-62.2879, -45.5095,  84.9430, -55.7443, -66.7292, -14.2052,  11.3388,\n",
       "           79.5401, -26.7521, -45.2004, -73.5535, -60.8900,  61.2795,  76.3788,\n",
       "           81.0477,   5.5627,  79.7452,  88.4933,  79.4474, -52.4423,  -6.2424,\n",
       "          -77.4395,  84.5068,  -5.8128,  69.0025,  -9.2078,  34.4233,  47.5806,\n",
       "          -56.3123,  45.1274,  24.4512, -67.4756,   7.0369,  69.3248, -79.3103,\n",
       "          -84.3746, -28.1447, -42.2293, -49.8444,  52.1058,  53.3185,  59.4978,\n",
       "           49.5530, -49.1463,   6.7993,  43.5543,   2.0462, -40.2766, -86.4186,\n",
       "          -19.2819, -37.9330,  53.8272, -14.4812,  85.6877,  17.1255, -32.8440,\n",
       "          -79.9432,  76.3225, -73.5065, -82.3773, -48.5265, -45.3776,  10.4991,\n",
       "          -11.9142, -11.2462, -34.4602, -19.7407, -64.6576, -69.7041, -58.3151,\n",
       "          -70.2743,  24.6584,   6.1128, -68.8005, -84.9105, -77.3439, -32.2949,\n",
       "          -31.9152, -66.0816, -51.4607, -83.3612,  -6.2227,  18.1654,  -1.0204,\n",
       "           -7.1122,  26.7413, -13.3628,  11.6045, -15.8711,   4.1956, -18.9523,\n",
       "          -31.2856, -20.7690,   8.0432,  -9.4208, -54.8239, -63.4671,  86.7185,\n",
       "            8.8260, -57.4866,  84.9146, -66.1095,  70.9283, -68.1896,  -4.5161,\n",
       "          -16.3451, -61.4246, -35.8322,  61.8628,  55.1104, -29.2081, -88.2048,\n",
       "          -46.1820,  71.7481,  79.4553, -40.5819,  63.9617, -49.2439, -66.4157,\n",
       "            7.1973, -54.0018, -43.5090, -70.4798, -73.2252, -64.2196,  22.3185,\n",
       "           -7.9575,  75.4633,  52.4451,  59.3509,  86.0923, -71.7111, -31.9321,\n",
       "          -68.9857,  39.6818,  20.1413, -76.0135,  38.2581, -14.0013, -18.7798,\n",
       "            1.1534,  23.7015,  50.2363, -55.5850, -30.5119,  24.5462, -12.3178,\n",
       "           50.3432,  72.4559,  84.7118,  62.8570, -43.6745,  80.4854,   4.6819,\n",
       "          -83.0956,  24.6498,  -3.4737,  39.9753,  54.6378, -32.0939,  -3.1623,\n",
       "           39.5410, -70.2886,  33.2865,   8.0863, -79.1933,   5.3169, -44.3133,\n",
       "          -35.9075,  38.2211,  10.3064,  88.3208,  -2.9559,  26.4600, -44.0773,\n",
       "           27.9216,  -4.1603, -88.8501, -57.7304, -60.1946, -15.7558, -47.1020,\n",
       "            7.6877,  36.7140, -59.1140, -21.8158, -44.3166, -74.2228,  11.9790,\n",
       "          -71.5152, -35.3936, -64.4089,  34.8173,  86.5309, -41.6382, -82.8126,\n",
       "          -25.4960,  60.4730,  49.4922,  -3.5295]]),\n",
       " tensor([ 85.2310, -35.2522])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_weights, update_weights, update_delays = sim.aggregator.parse_updates(to_update_global)\n",
    "\n",
    "weighted_average(model_weights,update_weights)\n",
    "\n",
    "# w_avg = [\n",
    "#         # each point has two dimensions, and by stacking them we get 3 dimensions\n",
    "#         # instead of doing nasty transposing, we can just add matching dims for the weight\n",
    "#         (\n",
    "#             torch.stack(p) \n",
    "#             * update_weights.reshape([len(model_weights)] + [1] * p[0].ndim)\n",
    "#         ).sum(0)\n",
    "#         for p in zip(*model_weights)\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([200, 784]),\n",
       " torch.Size([200]),\n",
       " torch.Size([200, 200]),\n",
       " torch.Size([200]),\n",
       " torch.Size([2, 200]),\n",
       " torch.Size([2])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.shape for w in w_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from dataset import load_MNIST, load_CIFAR\n",
    "\n",
    "x_train,y_train,x_test,y_test = load_CIFAR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024, 100, 100]\n"
     ]
    }
   ],
   "source": [
    "from models import MLP, ConvNet  \n",
    "\n",
    "# model = MLP(\n",
    "#     in_features= 784,\n",
    "#     out_classes= 2,\n",
    "#     hidden_layers=[200,200]\n",
    "# )\n",
    "\n",
    "model = ConvNet(\n",
    "    in_features = [32,32],\n",
    "    out_classes = 2,\n",
    "    conv_shapes = [16,16],\n",
    "    kernel_sizes = [3,3],\n",
    "    hidden_layers = [100,100],\n",
    "    dropout = 0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5251, 0.4749],\n",
       "        [0.5019, 0.4981],\n",
       "        [0.5233, 0.4767],\n",
       "        [0.5273, 0.4727],\n",
       "        [0.5284, 0.4716],\n",
       "        [0.4918, 0.5082],\n",
       "        [0.5357, 0.4643],\n",
       "        [0.5172, 0.4828],\n",
       "        [0.5103, 0.4897],\n",
       "        [0.5257, 0.4743]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv_layers(x_train[:10]).flatten(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robust-fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
